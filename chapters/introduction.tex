\chapter{Introduction}

Memories formed in the brain emerge from intricate patterns of neural activity involving specific subgroups of neurons. A pioneering model exploring this complex phenomenon is Valiant's 2005 Neuroidal model, which represents memories as random subgraphs over a larger base graph modeling the connectivity of the brain \cite{valiant2005memorization}. Activating a certain proprotion of neurons in a memory causes the memory to fire, and effectively be retrieved from this network. Valiant provided two main algorithms - JOIN, for forming new memories, and LINK, for associating pre existing memories. One unique aspect of this model is that all memories, pre-existing and newly formed, must approximately have the same size to behave like `equal citizens' in the system. To enforce this, Valiant introduced a set of six equations that the memory size must follow. For a given configuration of the system, there are unique integral solutions to this system of equations that Valiant referred to as the \textit{replication factor}.  

Valiant investigated two versions the Neuroidal model - a disjoint version where memories do not intersect, and a more biologically plausible shared version where memories are allowed to intersect. The capacity of the disjoint version can be easily estimated - it is the number of neurons in the graph divided by the replication factor. To study the capacity of the shared memory model, Valiant introduced a notion of interference between memories. This refers to the unintended firing of one memory when another is activated, caused by overlapping subgroups of neurons. As interference accumulates from storing more memories, quality of retrieval degrades - false firing escalates, signaling the network has hit its memory capacity. Valiant left quantitative characterization of this capacity for future work.

Since then, formal analysis of the neuroidal framework's storage capabilities remains limited. Valiant himself reinvestigated the capacity of the Neuroidal model with respect to LINK in 2017. However, he did not analyze the capacity with respect to JOIN, which we consider more interesting as it is primary memory creation tool in the model \cite{valiant2017capacity}. Recently, Perrine empirically investigated the capacity of the Neuroidal model with respect to JOIN and provided some valuable insights into the problem \cite{perrine2023neural}. There also have been recent empirical investigations into capacity in the context of the Assembly Calculus, a descendant of Valiantâ€™s model that uses Project and Merge, two more advanced memory formation operations inspired by LINK and JOIN respectively \cite{xie2023skip}. However, broader open questions persist regarding formulating general capacity theories spanning diverse random graph based models of cognition that capture essential interference phenomena governing information storage.

In service of this goal, we take foundational steps in this paper toward a rigorous capacity formulation for overlapping subset models in terms of expected interference between memories. We start by providing precise definitions for memory capacity and interference for a system of subsets over a finite base set. In contrast with the complex memory generation process used in Valiant's and other contemporary models, we initially consider random subset insertion to enable simpler closed-form solutions that can also be updated to account for the intricacies of different memory generation algorithms. Under simplifying assumptions, we derive expressions characterizing capacity, roughly defined as the maximum number of subsets that can be stored in the system before expected interference from adding more memories breaches intolerable thresholds. We also simulate the Neuroidal model inspired by Perrine's simulation in his thesis and swap out JOIN with random subset insertion to empirically validate our results \cite{perrine2023neural}. 

While mathematically convenient for an initial analysis, we understand random memory formation lacks biological plausibility. Therefore, we discuss strategies to adapt our interference calculations to represent specialized memory creation algorithms used in existing neural models, without compromising the generality of our overall capacity theory. As a case study, we analyze capacity in the Neuroidal model with respect to the JOIN operation. We simulate memory formation under JOIN, gaining preliminary insights into challenges to adapting our theory. Findings reveal uneven accumulation of interference on certain neurons, in contrast to the simplifying uniformity assumptions in our derivations.

Overall, this work initiates rigorous groundwork to elucidate the memory storage limitations of neural systems in light of interference. We substantiate our formulations with simulations that validate capacity findings under simplifying assumptions of random memory formation. The analytical capacity expressions and strategies proposed to handle complex memory creation algorithms offer potential springboards to tackle outstanding questions in exploring storage dynamics of contemporary cognitive models. They provide formal bases to investigate applications to long-standing frameworks like Valiant's neuroidal model and active areas like the Assembly Calculus.