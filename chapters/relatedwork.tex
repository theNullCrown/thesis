\chapter{Related work}

In this chapter, we go over works that have investigated the notion of capacity in random graph based models of cognition. To the best of our knowledge, there is a very limited amount of prior research carried out in this area. 

One of the first attempts to estimate the capacity of the random graph based models of cognition was undertaken by Valiant himself. Valiant concludes that it is complicated to derive the capacity of the model with shared memory representation, a sentiment we agree with. He accounts for interference by adding error rates for JOIN and LINK and the general noise rate $\sigma$, the total number of nodes active in the circuit at a given time, to his set of 6 equations governing JOIN and solves them assuming a reasonable bound on this value. Valiant also claimed that a single value for number of items that can be represented does not make sense for such a complicated model and it is more appropriate to simply bound the interference \cite{valiant2005memorization}. 

We assume a more optimistic stance regarding this and believe that it is possible to find an analytical formulation that will answer this question. We also think that Valiant agrees with us as he revisited this problem after a brief period of time, as discussed below, and made considerable progress in this area. While we are not able to solve the analytical capacity of the Neuroidal model with respect to shared JOIN in this paper, we believe we have laid significant groundwork for it and are optimistic it will be solved in the near future.

Valiant revisited the notion of capacity 4 years later in his 2009 paper ``Experience-Induced Neural Circuits That Achieve High Capacity''. He uses a relatively simple simulation of the Neuroidal model to analyze the capacity with a mix of tasks like memorization, association, inductive learning and hierarchical memory formation. He works with a rather loosely defined notion of interference in this paper that is however very similar to the idea introduced in 2005. The definition of capacity remains essentially the same as the point where there is too much interference or `degradation' \cite{feldman2009experience}. The empirical results provided in this paper are quite interesting to us however the use of a bipartite Neouroidal model makes it slightly less biologically plausible. In our simulations we work with general sets and random graphs with no further assumption of structure. 

Valiant again took on the challenge of capacity in his 2017 paper ``Capacity of Neural Networks for Lifelong Learning of Composable Tasks'', this time from a more theoretical standpoint. In this paper, he analyzed the capacity of the Neuroidal model with respect to LINK. Valiant was able to successfuly derive theoretical estimates for the upper bound of the capacity. Valiant carries over the concept of interference from the 2005 paper and studies the evolution of the system until the associations created by LINK are no longer clearly defined and there is too much unintended excitation or firing of other memories, in other words, there is too much interference in the system \cite{valiant2017capacity}. 

This paper serves as the primary inspiration for our work and we follow very similar theoretical tools and style to arrive at our results. Of primary interest here, is the fact that Valiant analyzes capacity with regards to an operation that does not create new memories \cite{valiant2017capacity}. We assume that Valiant intended for the system to evolve by JOINing existing pairs of memories to form new memories as that is the primary memorization algorithm associated to the Neuroidal model. This raises the question of whether the capacity upper bound derived in this paper is actually realizable or will the system reach the point of excess interference before that. This is the core reason for developing our theory with the memory formation algorithm as the central piece that determines the final formulation of interference and capacity.

In his 2023 paper, ``Neural Tabula Rasa: Foundations for Realistic Memories and Learning'', Perrine simulates the Neuroidal model and analyzes its capacity empirically with respect to the basic parameters of the model. This paper inspired us to experimentally verify our theoretical results using an adapted version of the simulation code provided by Perrine. We also find the results in this paper intriguing, especially the behavior of the model where the capacity increases with a higher number of pre-existing starting memories in the model. This goes against our conventional wisdom and we try to explain it in section 5.3.1 \cite{perrine2023neural}.

Also in 2023, Yi Xie, Yichen Li, and Akshay Rangamani explored the capacity of the Assembly Calculus with respect to the Project operation in their paper ``Skip Connections Increase the Capacity of Associative Memories in Variable Binding Mechanisms''. They work with a very similar interference driven definition of capacity without explicitly defining interference. The primary distinction with Neuroidal model based investigations into capacity is that they focus on interference between classes of memories rather than memories themselves. They define capacity as the number of classes when the within-class similarity is less than or equal to the between-class similarity. They were able to derive empirical results for this concept of capacity with respect to multiple basic parameters of the Assembly Calculus. Further, they also propose changes to some operations of the model to improve the capacity \cite{xie2023skip}. We believe this is a very important work in this area and along with Perrine's paper signals the growing interest in the notion of capacity in models of cognition.




